# -*- coding: utf-8 -*-
"""Final Mini-Project 2 Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SYElxI7484JKGITAk5hSnD01wZY2N5eX

**Import Libraries**
"""

!pip install yfinance

import numpy as np
import pandas as pd
from scipy.optimize import minimize
import yfinance as yf
from statsmodels.tsa.stattools import coint
from tqdm import tqdm
from datetime import datetime, timedelta
import itertools
from tqdm.notebook import tqdm
import os
import random
from collections import defaultdict

import warnings
warnings.filterwarnings("ignore")

import matplotlib.pyplot as plt
# set fig fontsize
plt.rcParams.update({"font.size": 14})
plt.rcParams["legend.fontsize"] = 11

# Set random seed for reproducibility
np.random.seed(42)

"""**Get ten years of historical S&P500 data**"""

sp500 = yf.Ticker("^GSPC")

start_date = "2014-01-01"
end_date = "2024-10-24"

sp500 = yf.Ticker("^GSPC")
sp500_past_performance = sp500.history(start=start_date, end=end_date)

plt.figure(figsize=(10,6))
plt.plot(sp500_past_performance['Close'], label='S&P 500 Closing Price')
plt.title('S&P 500 Historical Closing Prices')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

# tickers for all stocks
sp500_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
sp500_df = pd.read_html(sp500_url)[0]

# Exclude delisted tickers & companies founded after 2013
sp500_df = sp500_df[(sp500_df.Symbol != 'BRK.B') & (sp500_df.Symbol != 'BF.B')]
exclude_list = ['CEG', 'GEHC', 'SW','VLTO', 'GEV', 'SOLV', 'AMTM', 'KVUE','CTAS']
sp500_df = sp500_df[sp500_df.Symbol.isin(exclude_list) == False]

all_tickers = sp500_df['Symbol'].tolist()
corresponding_sectors = sp500_df['GICS Sector'].tolist()

# Create a dict mapping each ticker to its sector
stock_and_sectors_dict = {}

for i in range(len(all_tickers)):
    stock_and_sectors_dict[all_tickers[i]] = corresponding_sectors[i]

print(dict(itertools.islice(stock_and_sectors_dict.items(), 5)))
print(all_tickers[:5])
print(f'Total number of tickers: {len(all_tickers)}')

closing_prices_df = yf.download(all_tickers, start=start_date, end=end_date)["Adj Close"]

issue_columns = closing_prices_df.columns[closing_prices_df.isna().any()].tolist()
closing_prices_df = closing_prices_df.drop(issue_columns, axis=1)

filtered_filename = '/content/closing_prices.csv'
closing_prices_df.to_csv(filtered_filename, index=True)

# Get daily returns
daily_returns_df = closing_prices_df.pct_change().dropna()
returns_filename = '/content/daily_returns.csv'
daily_returns_df.to_csv(returns_filename, index=True)

"""**Run Cointegration Test to Determine Mean-Reverting Stock Pairs**"""

closing_prices_df = pd.read_csv('/content/closing_prices.csv',index_col=0)
daily_returns_df = pd.read_csv('/content/daily_returns.csv',index_col=0)

"""**Based on historical cointegration, create a list of pairs with mean reverting behavior. Only need to run this section once.**"""

batch_size = 10000
save_filename = '/content/cointegrated_pairs.csv'
progress_filename = '/content/processing_progress.csv'
processed_pairs = set()
closing_prices_df = pd.read_csv('/content/closing_prices.csv')

if os.path.exists(save_filename):
    saved_df = pd.read_csv(save_filename)
    processed_pairs = set(zip(saved_df['Stock 1'], saved_df['Stock 2']))
else:
    saved_df = pd.DataFrame(columns=['Stock 1', 'Stock 2', 'P-Value'])

if os.path.exists(progress_filename):
    progress_df = pd.read_csv(progress_filename)
    pairs_processed = progress_df['Pairs Processed'].iloc[0]
else:
    pairs_processed = 0  # Start from zero if no progress file exists

new_cointegrated_pairs = []

stock_pairs = [pair for pair in itertools.combinations(closing_prices_df.columns, 2) if pair not in processed_pairs]

if pairs_processed >= len(stock_pairs):
    print("All pairs have been processed.")
else:
    batch = stock_pairs[pairs_processed:pairs_processed + batch_size]

    for stock_1, stock_2 in tqdm(batch, desc=f"Processing batch, starting from pair {pairs_processed + 1}"):
        t_score, p_value, critical_value = coint(closing_prices_df[stock_1], closing_prices_df[stock_2])

        # Check if the p-value is significant
        if p_value < 0.05:
            new_cointegrated_pairs.append((stock_1, stock_2, p_value))

        processed_pairs.add((stock_1, stock_2))

    if new_cointegrated_pairs:
        new_df = pd.DataFrame(new_cointegrated_pairs, columns=['Stock 1', 'Stock 2', 'P-Value'])
        new_df.to_csv(save_filename, mode='a', header=not os.path.exists(save_filename), index=False)
        new_cointegrated_pairs.clear()

    pairs_processed += batch_size
    pd.DataFrame({'Pairs Processed': [pairs_processed]}).to_csv(progress_filename, index=False)

    # Update user
    print(f"Batch complete. {pairs_processed} pairs processed in total. Results saved to '{save_filename}'.")

cointegrated_pairs_df = pd.read_csv('/content/cointegrated_pairs.csv')
sorted_pairs = cointegrated_pairs_df.sort_values(by='P-Value', ascending=True)
sorted_pairs.reset_index(drop=True, inplace=True)
sorted_pairs.to_csv('/content/sorted_cointegrated_pairs.csv', index=False)

"""**Rerun cointegration test over shorter two year window.**"""

initial_pairs = pd.read_csv('/content/sorted_cointegrated_pairs_original.csv')
unique_stocks = pd.unique(initial_pairs[['Stock 1', 'Stock 2']].values.ravel('K')).tolist()

start_date = '2014-01-01'
end_date = '2024-10-24'

shortlist_prices = yf.download(unique_stocks, start=start_date, end=end_date)["Adj Close"]
shortlist_prices.dropna(how='all', inplace=True)
shortlist_prices.index = shortlist_prices.index.tz_localize(None)

# Identify cointegrated pairs over a 2-year lookback period
def get_cointegrated_pairs(pairs, end_date, price_data, p_threshold=0.05):
    end = datetime.strptime(end_date, '%Y-%m-%d')
    start = end - timedelta(days=2*365)
    sliced_data = price_data.loc[start:end]

    results = []
    for _, row in tqdm(pairs.iterrows(), total=len(pairs), desc="Processing pairs"):
        stock1, stock2 = row['Stock 1'], row['Stock 2']

        if stock1 in sliced_data.columns and stock2 in sliced_data.columns:
            combined_data = sliced_data[[stock1, stock2]].dropna()

            if not combined_data.empty:
                _, p_value, _ = coint(combined_data[stock1], combined_data[stock2])

                if p_value < p_threshold:
                    results.append({'Stock 1': stock1, 'Stock 2': stock2, 'P-Value': p_value})

    return pd.DataFrame(results).reset_index(drop=True)

cointegrated_pairs = get_cointegrated_pairs(initial_pairs, '2024-01-01', shortlist_prices)

"""**Filter each stock by volatility and volume.**"""

# Volume data
shortlist_volume = yf.download(unique_stocks, start=start_date, end=end_date)["Volume"]
shortlist_volume.dropna(how='all', inplace=True)
shortlist_volume.index = shortlist_volume.index.tz_localize(None)

def quality_filter(pairs, price_data, volume_data, end_date, volume_threshold=500000, volatility_threshold=0.025):
    start_date = end_date - timedelta(days=2*365)

    sliced_price_data = price_data.loc[start_date:end_date]
    sliced_volume_data = volume_data.loc[start_date:end_date]
    filtered_pairs = []

    for _, row in pairs.iterrows():
        stock1, stock2 = row['Stock 1'], row['Stock 2']

        if stock1 in sliced_price_data.columns and stock2 in sliced_price_data.columns:
            returns1 = sliced_price_data[stock1].pct_change().dropna()
            returns2 = sliced_price_data[stock2].pct_change().dropna()

            avg_volume1 = sliced_volume_data[stock1].mean() if stock1 in sliced_volume_data.columns else 0
            avg_volume2 = sliced_volume_data[stock2].mean() if stock2 in sliced_volume_data.columns else 0

            volatility1 = returns1.std()
            volatility2 = returns2.std()

            if (avg_volume1 > volume_threshold and avg_volume2 > volume_threshold and
                volatility1 < volatility_threshold and volatility2 < volatility_threshold):
                filtered_pairs.append({
                    'Stock 1': stock1,
                    'Stock 2': stock2,
                    'P-Value': row['P-Value'],
                    'Avg Volume 1': avg_volume1,
                    'Avg Volume 2': avg_volume2,
                    'Volatility 1': volatility1,
                    'Volatility 2': volatility2
                })

    filtered_pairs = pd.DataFrame(filtered_pairs)
    filtered_pairs = filtered_pairs.sort_values(by='P-Value', ascending=True)
    filtered_pairs.reset_index(drop=True, inplace=True)

    return filtered_pairs

"""**Rank the remaining pairs by their profit to risk ratio**"""

# Rank pairs by profit-to-risk ratio (average spread return / spread volatility) within a 2-year period.
def rank_profitability(pairs, price_data, end_date):
    start_date = end_date - timedelta(days=2*365)

    sliced_data = price_data.loc[start_date:end_date]
    profitability_data = []

    for _, row in pairs.iterrows():
        stock1, stock2 = row['Stock 1'], row['Stock 2']

        if stock1 in sliced_data.columns and stock2 in sliced_data.columns:
            spread = sliced_data[stock1] - sliced_data[stock2]
            spread_returns = spread.pct_change().dropna()
            avg_spread_return = spread_returns.mean()
            spread_volatility = spread_returns.std()

            profit_to_risk = abs(avg_spread_return) / spread_volatility if spread_volatility > 1e-6 else 0

            profitability_data.append({
                'Stock 1': stock1,
                'Stock 2': stock2,
                'P-Value': row['P-Value'],
                'Avg Spread Return': avg_spread_return,
                'Spread Volatility': spread_volatility,
                'Profit-to-Risk Ratio': profit_to_risk
            })

    profitability_df = pd.DataFrame(profitability_data)
    profitability_df.sort_values(by='Profit-to-Risk Ratio', ascending=False, inplace=True)

    return profitability_df.reset_index(drop=True)

"""**Filter out too closely correlated pairs to ensure diversification**"""

# Filter pairs by pairwise correlation of spread returns
def correlation_filter(ranked_pairs, price_data, end_date, correlation_threshold=0.8):
    start_date = end_date - timedelta(days=2*365)

    sliced_data = price_data.loc[start_date:end_date]
    selected_pairs = []
    spread_data = {}

    for _, row in ranked_pairs.iterrows():
        stock1, stock2 = row['Stock 1'], row['Stock 2']

        spread = sliced_data[stock1] - sliced_data[stock2]
        spread_returns = spread.pct_change().dropna()
        spread_data[(stock1, stock2)] = spread_returns

    spread_df = pd.DataFrame(spread_data)
    correlation_matrix = spread_df.corr()

    for pair in spread_data:
        is_diversified = True
        for selected_pair in selected_pairs:
            if abs(correlation_matrix.loc[pair, selected_pair]) > correlation_threshold:
                is_diversified = False
                break
        if is_diversified:
            selected_pairs.append(pair)

    selected_pairs_df = ranked_pairs[ranked_pairs[['Stock 1', 'Stock 2']].apply(tuple, axis=1).isin(selected_pairs)]
    return selected_pairs_df.reset_index(drop=True)

"""**Get the number of z-score triggers over a trading period**"""

# Filter pairs based on consistent z-score trigger frequency within a 2-year window
def signal_filter(pairs, price_data, end_date, threshold=1.75, min_triggers=6, max_triggers=30):
    start_date = end_date - timedelta(days=2*365)

    sliced_data = price_data.loc[start_date:end_date]
    consistent_pairs = []

    for _, row in pairs.iterrows():
        stock1, stock2 = row['Stock 1'], row['Stock 2']

        if stock1 in sliced_data.columns and stock2 in sliced_data.columns:
            spread = sliced_data[stock1] - sliced_data[stock2]
            spread_mean = spread.mean()
            spread_std = spread.std()
            z_scores = (spread - spread_mean) / spread_std

            trigger_count = ((z_scores >= threshold) | (z_scores <= -threshold)).sum()

            if trigger_count >= min_triggers and trigger_count <= max_triggers:
                consistent_pairs.append({
                    'Stock 1': stock1,
                    'Stock 2': stock2,
                    'P-Value': row['P-Value'],
                    'Trigger Count': trigger_count
                })

    return pd.DataFrame(consistent_pairs).reset_index(drop=True)

"""**Core function that does all of the filtering in one**"""

def get_portfolio(filtered_pairs, price_data, end_date):

    # Rank pairs by profitability
    ranked_pairs = rank_profitability(filtered_pairs, price_data, end_date)

    # Apply correlation filtering for diversification
    diversified_pairs = correlation_filter(ranked_pairs, price_data, end_date)

    # Apply consistent signal frequency check
    consistent_pairs = signal_filter(diversified_pairs, price_data, end_date)

    # Reduce exposure to single stocks
    stock_count = defaultdict(int)
    final_pairs = []

    for _, row in consistent_pairs.iterrows():
        stock1, stock2 = row['Stock 1'], row['Stock 2']

        if stock_count[stock1] < 2 and stock_count[stock2] < 2:
            final_pairs.append(row)
            stock_count[stock1] += 1
            stock_count[stock2] += 1

        if len(final_pairs) >= 50:
            break

    final_pairs_df = pd.DataFrame(final_pairs).reset_index(drop=True)
    return final_pairs_df

"""**Allocate capital according to markowitz optimization**"""

# Calculate expected returns, variances, and covariances for the selected pairs
def get_stats(selected_pairs, price_data, end_date):
    start_date = end_date - timedelta(days=2*365)

    sliced_data = price_data.loc[start_date:end_date]
    spread_returns = {}

    for _, row in selected_pairs.iterrows():
        stock1, stock2 = row['Stock 1'], row['Stock 2']
        spread = sliced_data[stock1] - sliced_data[stock2]
        spread_returns[(stock1, stock2)] = spread.pct_change().dropna()

    spread_returns_df = pd.DataFrame(spread_returns)

    exp_returns = spread_returns_df.mean()
    cov_matrix = spread_returns_df.cov()

    return exp_returns, cov_matrix

# Perform Markowitz optimization to allocate capital across pairs with a 12.5% cap on individual weights
def markowitz_opt(exp_returns, cov_matrix, risk_free_rate, max_allocation=0.125):
    num_assets = len(exp_returns)

    # Negative Sharpe ratio
    def negative_sharpe(weights):
        portfolio_return = np.dot(weights, exp_returns)
        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
        sharpe_ratio = (portfolio_return - risk_free_rate) / portfolio_volatility
        return -sharpe_ratio

    constraints = [
        {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1},
    ]

    bounds = tuple((0, max_allocation) for _ in range(num_assets))
    initial_guess = num_assets * [1.0 / num_assets]

    result = minimize(negative_sharpe, initial_guess, bounds=bounds, constraints=constraints)

    return result.x

"""**Allocate capital based on weights**"""

def allocate_capital(selected_pairs, optimized_weights, total_capital):
    allocations = []

    for i, (_, row) in enumerate(selected_pairs.iterrows()):
        stock1, stock2 = row['Stock 1'], row['Stock 2']
        allocation = optimized_weights[i] * total_capital
        allocations.append({'Stock 1': stock1, 'Stock 2': stock2, 'Capital Allocation': allocation})

    return pd.DataFrame(allocations)

"""**Core function to optimize and allocate capital to portfolio pairs**"""

# 6 month treasury bill
risk_free_rate = .0443/252

def optimize_portfolio(selected_pairs, price_data, total_capital, end_date, risk_free_rate=risk_free_rate, min_allocation=0.01):

    exp_returns, cov_matrix = get_stats(selected_pairs, price_data, end_date=end_date)
    optimized_weights = markowitz_opt(exp_returns, cov_matrix, risk_free_rate=risk_free_rate)
    allocations = []
    for i, (_, row) in enumerate(selected_pairs.iterrows()):
        stock1, stock2 = row['Stock 1'], row['Stock 2']

        # Apply minimum allocation threshold
        pair_weight = optimized_weights[i] if optimized_weights[i] > min_allocation else 0

        pair_allocation = pair_weight * total_capital
        long_allocation = pair_allocation / 2
        short_allocation = pair_allocation / 2

        allocations.append({
            'Stock 1': stock1,
            'Stock 2': stock2,
            'Long Allocation': long_allocation,
            'Short Allocation': short_allocation,
            'Total Pair Allocation': pair_allocation
        })

    return pd.DataFrame(allocations)

"""**Filter out pairs with zero allocation**"""

def filter_allocated_pairs(capital_allocations):
    return capital_allocations[capital_allocations['Total Pair Allocation'] > 0].reset_index(drop=True)

"""**Get z-scores, which will act as signals for trading**"""

def calculate_z_score(spread):
    spread_mean = spread.mean()
    spread_std = spread.std()
    return (spread - spread_mean) / spread_std

"""**Generate entry and exit signals based on z-scores**"""

def generate_signals(allocated_pairs, stock_data, active_trades, trade_start, z_entry_threshold=1.75, z_exit_threshold=1.5, z_stop_loss=2.75):
    if isinstance(trade_start, str):
        start_date = datetime.strptime(trade_start, '%Y-%m-%d')
    else:
        start_date = trade_start
    window_start_date = start_date - timedelta(days=2*365)

    trade_end = trade_start + pd.Timedelta(days=180)
    signals = []
    for _, row in allocated_pairs.iterrows():
        stock1 = row['Stock 1']
        stock2 = row['Stock 2']

        stop_loss_count = 0
        spread = stock_data[stock1] - stock_data[stock2]

        for current_date in spread.loc[trade_start:trade_end].index:
            rolling_window = spread.loc[window_start_date:current_date]
            current_z = calculate_z_score(rolling_window).iloc[-1]

            # Long stock 2, short stock 1
            if active_trades.empty and stop_loss_count <=2:
                if current_z >= z_entry_threshold and current_z <= z_stop_loss:
                    signals.append({
                        'Long': stock2,
                        'Short': stock1,
                        'Z-Score': current_z,
                        'Signal': 'Enter',
                        'Date': current_date
                    })

                    active_trades.loc[len(active_trades)] = {'Stock 1': stock1, 'Stock 2': stock2, 'Entry Z-Score': current_z}

                # Long stock 1, short stock 2
                elif current_z <= -z_entry_threshold and current_z >= -z_stop_loss:
                    signals.append({
                        'Long': stock1,
                        'Short': stock2,
                        'Z-Score': current_z,
                        'Signal': 'Enter',
                        'Date': current_date
                    })

                    active_trades.loc[len(active_trades)] = {'Stock 1': stock1, 'Stock 2': stock2, 'Entry Z-Score': current_z}

            elif stop_loss_count >=2:
                break

            else:
                entry_z_score = active_trades['Entry Z-Score'].values[-1]

                if entry_z_score > 0:
                    long = stock2
                    short = stock1
                else:
                    long = stock1
                    short = stock2

                # Take profit on trade
                if abs(current_z) <= z_exit_threshold:
                    signals.append({
                        'Long': long,
                        'Short': short,
                        'Z-Score': current_z,
                        'Signal': 'Exit (Take Profit)',
                        'Date': current_date
                    })

                    active_trades = pd.DataFrame(columns=['Stock 1', 'Stock 2', 'Entry Z-Score'])

                # Cut losses on trade
                elif abs(current_z) >= z_stop_loss:
                    signals.append({
                        'Long': long,
                        'Short': short,
                        'Z-Score': current_z,
                        'Signal': 'Exit (Stop Loss)',
                        'Date': current_date
                    })
                    active_trades = pd.DataFrame(columns=['Stock 1', 'Stock 2', 'Entry Z-Score'])

                    stop_loss_count+=1

        active_trades = pd.DataFrame(columns=['Stock 1', 'Stock 2', 'Entry Z-Score'])

    return pd.DataFrame(signals)

"""**Assign correct amount of capital to long and short positions of each pairs trade on signal**"""

def signals_and_amounts(signals, allocated_pairs):
    long_allocations = []
    short_allocations = []

    for _, row in signals.iterrows():
        if row['Signal'] == 'Enter':
            long_stock = row['Long']
            short_stock = row['Short']

            stocks = [long_stock, short_stock]
            total_pair_allocation = allocated_pairs.loc[allocated_pairs['Stock 1'].isin(stocks), 'Total Pair Allocation']

            long_allocation = total_pair_allocation.iloc[0] / 2
            long_allocations.append(long_allocation)

            short_allocation = total_pair_allocation.iloc[0] / 2
            short_allocations.append(short_allocation)

        else:
            long_allocations.append('N/A')
            short_allocations.append('N/A')

    signals['Long Allocation'] = long_allocations
    signals['Short Allocation'] = short_allocations

    return signals

"""**Get the total returns made on each trading opportunity**"""

def calc_arbitrage_returns(signals_and_amounts, price_data, trade_start):
    trade_start = pd.to_datetime(trade_start)
    trade_end = trade_start + pd.Timedelta(days=180)
    trade_end = min(price_data.index, key=lambda x: abs(x - trade_end))

    total_return = []
    percentage_return = []

    for index, row in signals_and_amounts.iterrows():
        if row['Signal'] == 'Enter':
            entry_date = row['Date']
            long_stock = row['Long']
            short_stock = row['Short']
            entry_allocation_long = row['Long Allocation']
            entry_allocation_short = row['Short Allocation']

            # Record entry prices
            entry_price_long = price_data.loc[entry_date, long_stock]
            entry_price_short = price_data.loc[entry_date, short_stock]

            if index + 1 < len(signals_and_amounts):
                next_row = signals_and_amounts.iloc[index+1]

                # close at end of time period if not before
                if next_row['Signal'] == 'Enter':
                    exit_date = trade_end

                else:
                    exit_date = next_row['Date']

            else:
                exit_date = trade_end

            close_long_price = price_data.loc[exit_date, long_stock]
            close_short_price = price_data.loc[exit_date, short_stock]

            long_pct_change = (close_long_price - entry_price_long) / entry_price_long
            short_pct_change = (close_short_price - entry_price_short) / entry_price_short

            long_return = long_pct_change * entry_allocation_long
            short_return = -short_pct_change * entry_allocation_short

            trade_return = long_return + short_return

            total_return.append(trade_return)

            # Calculate percentage return
            total_capital_allocated = entry_allocation_long + entry_allocation_short
            if total_capital_allocated > 0:
                percent_return = (trade_return / total_capital_allocated) * 100
            else:
                percent_return = 0

            percentage_return.append(percent_return)

        else:
            trade_return = 0
            total_return.append(trade_return)
            percent_return = 0
            percentage_return.append(percent_return)


    signals_and_amounts['Trade Profits'] = total_return
    signals_and_amounts['Percentage Return'] = percentage_return

    # Sum up all the individual trade returns to get the total return
    total_value_of_returns = sum([ret for ret in total_return if isinstance(ret, (int, float))])

    return signals_and_amounts, total_value_of_returns

"""**Compare Returns on Active Trading Days with Returns That Could Have Been Made on the SP500**"""

# ingnore compounding and just focus on percentage change in returns
# assume that capital not being actively used in a trade can be allocated elsewhere (ex: risk-free investment, SP500)

start_date = '2024-01-01'
end_date = (datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=180)).strftime('%Y-%m-%d')
SP_500_ticker = '^GSPC'
SP_500_closing_prices = yf.download(SP_500_ticker, start=start_date, end=end_date)["Adj Close"]
SP_500_closing_prices.index = SP_500_closing_prices.index.tz_localize(None)

# Calculate the returns that could have been made on the SP_500 during the trade period using the same weights and trading on the same days.
def calc_alt_returns(arbitrage_returns, SP_500_closing_prices, trade_start, trade_end):

    trade_end = min(SP_500_closing_prices.index, key=lambda x: abs(x - trade_end))
    SP_500_returns = []

    for index, row in arbitrage_returns.iterrows():
        if row['Signal'] == 'Enter':
            entry_date = row['Date']
            total_allocation = row['Long Allocation'] + row['Short Allocation']

            open_price = SP_500_closing_prices.loc[entry_date, '^GSPC']

            if index + 1 < len(arbitrage_returns):
                next_row = arbitrage_returns.iloc[index + 1]

                if next_row['Signal'] == 'Enter':
                    exit_date = trade_end
                else:
                    exit_date = next_row['Date']
            else:
                exit_date = trade_end

            close_price = SP_500_closing_prices.loc[exit_date, '^GSPC']

            SP_500_pct_change = (close_price - open_price) / open_price
            SP_500_return = SP_500_pct_change * total_allocation

            SP_500_returns.append(SP_500_return)
        else:
            SP_500_returns.append(0)

    arbitrage_returns['Alt SP500 Profits'] = SP_500_returns
    total_SP_500_returns = sum(SP_500_returns)

    return arbitrage_returns, total_SP_500_returns

"""**Call the main function**"""

index_start = '2014-01-01'
index_end = '2024-01-01'

SP_500_ticker = '^GSPC'
index_closing_prices = yf.download(SP_500_ticker, start=index_start, end=index_end)["Adj Close"]
index_closing_prices.index = index_closing_prices.index.tz_localize(None)

# Start and end dates for the training
start_date = "2016-01-01"
end_date = "2024-01-01"

# Parameters already loaded
pairs = initial_pairs
price_data = shortlist_prices
volume_data = shortlist_volume
total_capital = 1000000

# Thresholds for cointegration and filtering
p_threshold = 0.05
volume_threshold = 500000
volatility_threshold = 0.025
min_allocation = 0.01

# Z-score thresholds for signal generation
z_entry_threshold = 1.75
z_exit_threshold = 1.5
z_stop_loss = 2.75

# Arbitrage trading bot – sixteen 6-month trading sessions
def main(start_date, end_date, pairs, price_data, volume_data, total_capital, index_closing_prices,
         risk_free_rate=risk_free_rate, p_threshold=p_threshold, volume_threshold=volume_threshold,
         volatility_threshold=volatility_threshold, min_allocation=min_allocation, z_entry_threshold=z_entry_threshold,
         z_exit_threshold=z_exit_threshold, z_stop_loss=z_stop_loss):

    current_date = start_date
    cumulative_arbitrage_profit = 0
    cumulative_sp500_profit = 0
    period_results = []
    trade_results = []
    session_count = 0

    while current_date < end_date:
        # Stop after 16 sessions
        if session_count >= 16:
            break

        # Identify cointegrated pairs
        cointegrated_pairs = get_cointegrated_pairs(pairs, current_date.strftime('%Y-%m-%d'), price_data, p_threshold)

        # Apply volume and volatility quality filtering
        filtered_pairs = quality_filter(cointegrated_pairs, price_data, volume_data,
                                        current_date, volume_threshold, volatility_threshold)

        if filtered_pairs.empty:
            print(f"No suitable pairs found for the period ending {current_date}")
            current_date += timedelta(days=180)
            continue

        # Filter with remaining criteria
        final_pairs = get_portfolio(filtered_pairs, price_data, current_date)

        if final_pairs.empty:
            print(f"No final pairs available for the period ending {current_date}")
            current_date += timedelta(days=180)
            continue

        # Optimize portfolio and allocate capital
        allocations_df = optimize_portfolio(final_pairs, price_data, total_capital,
                                            end_date=current_date,
                                            risk_free_rate=risk_free_rate,
                                            min_allocation=min_allocation)

        filtered_allocations_df = filter_allocated_pairs(allocations_df)
        if filtered_allocations_df.empty:
            print(f"No allocated pairs with non-zero capital for the period ending {current_date}")
            current_date += timedelta(days=180)
            continue

        # Generate entry and exit signals for each trading period
        trade_start = current_date
        period_end_date = min(current_date + timedelta(days=180), end_date)
        active_trades = pd.DataFrame(columns=['Stock 1', 'Stock 2', 'Entry Z-Score'])
        signals_df = generate_signals(filtered_allocations_df, price_data, active_trades, trade_start,
                                      z_entry_threshold=z_entry_threshold, z_exit_threshold=z_exit_threshold,
                                      z_stop_loss=z_stop_loss)

        # Calculate trade amounts for each signal
        signals_with_amounts = signals_and_amounts(signals_df, filtered_allocations_df)

        # Calculate arbitrage returns
        returns_df, total_session_return = calc_arbitrage_returns(signals_with_amounts, price_data, trade_start)
        trade_results.append(returns_df)
        cumulative_arbitrage_profit += total_session_return
        returns_df["Cumulative Arbitrage Profit"] = cumulative_arbitrage_profit

        # Calculate alternative SP500 returns for this period
        returns_df, total_sp500_return = calc_alt_returns(returns_df, index_closing_prices, trade_start, period_end_date)
        cumulative_sp500_profit += total_sp500_return
        returns_df["Cumulative SP500 Profit"] = cumulative_sp500_profit

        period_results.append({
            "period_start": current_date,
            "period_end": period_end_date,
            "arbitrage_profit": total_session_return,
            "cumulative_arbitrage_profit": cumulative_arbitrage_profit,
            "sp500_profit": total_sp500_return,
            "cumulative_sp500_profit": cumulative_sp500_profit
        })

        print(f"Completed trading period from {current_date} to {period_end_date} with arbitrage profit: {total_session_return}, cumulative arbitrage profit: {cumulative_arbitrage_profit}, cumulative SP500 profit: {cumulative_sp500_profit}")

        # Advance the current date by 180 days for the next iteration
        current_date = period_end_date
        session_count += 1

    arbitrage_returns = pd.concat(trade_results, ignore_index=True)
    period_results_df = pd.DataFrame(period_results)

    return arbitrage_returns, period_results_df

arbitrage_returns, period_summary = main(
    start_date=start_date,
    end_date=end_date,
    pairs=pairs,
    price_data=price_data,
    volume_data=volume_data,
    total_capital=total_capital,
    index_closing_prices=index_closing_prices,
    risk_free_rate=risk_free_rate,
    p_threshold=p_threshold,
    volume_threshold=volume_threshold,
    volatility_threshold=volatility_threshold,
    min_allocation=min_allocation,
    z_entry_threshold=z_entry_threshold,
    z_exit_threshold=z_exit_threshold,
    z_stop_loss=z_stop_loss,
)

arbitrage_returns.to_csv('arbitrage_returns.csv', index=False)
period_summary.to_csv('period_summary.csv', index=False)

"""**Create plots for visualization**"""

arbitrage_returns = pd.read_csv('arbitrage_returns.csv')
period_summary = pd.read_csv('period_summary.csv')

# plot the return per period for the arbitrage model and the SP500 case
dates = period_summary['period_start'] + " - " + period_summary['period_end']
arbitrage_profits = period_summary['arbitrage_profit']
sp500_profits = period_summary['sp500_profit']

fig, ax = plt.subplots(figsize=(10, 6))
width = 0.4
ax.bar(np.arange(len(dates)) - width / 2, arbitrage_profits, width=width, label="Arbitrage Profits Per 6-Months", alpha=.7)
ax.bar(np.arange(len(dates)) + width / 2, sp500_profits, width=width, label="S&P 500 Profits Per 6-Months", alpha=.7)
ax.set_xticks(np.arange(len(dates)))
ax.set_xticklabels(dates, rotation=30, ha="right")
ax.tick_params(axis='x', labelsize=10)
ax.set_xlabel('6-Month Trading Windows')
ax.set_ylabel('Trade Profits (in $)')
ax.set_title('Profits Per 6-Month Trading Period For Arbitrage Model vs. S&P 500')
ax.legend()
plt.show()

# plot the cumulative returns over time for our model and the SP500 case on the same plot
dates = period_summary['period_start'] + " - " + period_summary['period_end']
arbitrage_profits_cumulative = period_summary['cumulative_arbitrage_profit']
sp500_profits_cumulative = period_summary['cumulative_sp500_profit']

fig, ax = plt.subplots(figsize=(10, 6))
width = 0.4
ax.bar(np.arange(len(dates)) - width / 2, arbitrage_profits_cumulative, width=width, label="Cumulative Arbitrage Profits", alpha=.7)
ax.bar(np.arange(len(dates)) + width / 2, sp500_profits_cumulative, width=width, label="Cumulative S&P 500 Profits", alpha=.7)
ax.set_xticks(np.arange(len(dates)))
ax.set_xticklabels(dates, rotation=45, ha="right")
ax.tick_params(axis='x', labelsize=8)
ax.set_xlabel('6-Month Trading Windows')
ax.set_ylabel('Cumulative Profits')
ax.set_title('Cumulative Profits Over Time For Arbitrage Model vs. S&P 500')
ax.legend()
plt.show()

# same plot as above but as a line plot instead
plt.figure(figsize=(10,6))
plt.plot(dates, arbitrage_profits_cumulative, label='Cumulative Arbitrage Profits', color='blue')
plt.plot(dates, sp500_profits_cumulative, label='Cumulative S&P 500 Profits', color='orange')
plt.title('Cumulative Profits Over Time For Arbitrage Model vs. S&P 500')
plt.xticks(rotation=30, ha='right')
plt.xticks(fontsize=10)
plt.xlabel('6-Month Trading Windows')
plt.ylabel('Cumulative Profits')
plt.legend()
plt.show()